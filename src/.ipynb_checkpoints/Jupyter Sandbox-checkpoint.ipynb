{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint as pp     # pretty printing module\n",
    "from matplotlib import pyplot as plt        # required only for plotting results\n",
    "from mondrianforest_utils import load_data, reset_random_seed, precompute_minimal \n",
    "from mondrianforest import process_command_line, MondrianForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./mondrianforest_demo.py --dataset satimage --n_mondrians 100 --budget -1 \n",
    "#    --normalize_features 1 --save 1 --data_path ../process_data/ --n_minibatches 10 \n",
    "#    --store_every 1 --optype class\n",
    "\n",
    "class settings:\n",
    "    alpha = 0\n",
    "    bagging = 0\n",
    "    budget= -1.0\n",
    "    budget_to_use= float(\"inf\")\n",
    "    data_path = 'C:\\\\Users\\\\kkenda\\\\Desktop\\\\moa\\\\moa\\\\src\\\\examples\\\\java\\\\moa\\\\experiments\\\\data/'\n",
    "    dataset = 'id12041022_1_AR_WF_DT.arff'\n",
    "    debug = 0\n",
    "    discount_factor = 10\n",
    "    draw_mondrian = 0\n",
    "    init_id = 1\n",
    "    min_samples_split = 2\n",
    "    n_minibatches = 4000\n",
    "    n_mondrians = 10\n",
    "    name_metric = 'mse'\n",
    "    normalize_features = 1\n",
    "    op_dir = 'results'\n",
    "    optype = 'real'\n",
    "    perf_dataset_keys = ['train', 'test']\n",
    "    perf_metrics_keys = ['log_prob', 'acc']\n",
    "    perf_store_keys = ['pred_prob']\n",
    "    save = 1\n",
    "    select_features = 0\n",
    "    smooth_hierarchically = 0\n",
    "    store_every = 0\n",
    "    tag = ''\n",
    "    verbose = 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkenda\\Desktop\\moa\\moa\\src\\examples\\java\\moa\\experiments\\data/id12041022_1_AR_WF_DT.arff\n"
     ]
    }
   ],
   "source": [
    "data = load_data(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param, cache = precompute_minimal(data, settings)\n",
    "mf = MondrianForest(settings, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "minibatch\tmetric_train\tmetric_test\tnum_leaves\n",
      "('Training 0 batch', 21)\n",
      "Finished training ...\n",
      "('Evaluation on batch', 1)\n",
      "log_prob (using Gaussian approximation) = -259.405184\n",
      "log_prob (using mixture of Gaussians) = -8.449451\n",
      "Averaging over all trees, mse = 81039.403688, rmse = 284.674206, log_prob = -8.449451\n",
      "(0, 143.9633397893509, 171.0)\n",
      "(1, 144.39062197609542, 192.0)\n",
      "(2, 143.80088929318759, 198.0)\n",
      "(3, 140.00386224374608, 237.0)\n",
      "(4, 137.1929977605318, 340.0)\n",
      "(5, 136.1536968812236, 403.0)\n",
      "(6, 133.35935171563386, 401.0)\n",
      "(7, 133.04602046091335, 450.0)\n",
      "(8, 132.92351363868249, 458.0)\n",
      "(9, 132.8531855864785, 453.0)\n",
      "(10, 132.78947824538639, 463.0)\n",
      "(11, 132.65662280804278, 463.0)\n",
      "(12, 133.21454264318197, 412.0)\n",
      "(13, 133.77948180192402, 448.0)\n",
      "(14, 133.54498560598367, 467.0)\n",
      "(15, 132.62197260970098, 480.0)\n",
      "(16, 132.73764073535762, 496.0)\n",
      "(17, 132.30588020080759, 494.0)\n",
      "(18, 132.63951901529322, 500.0)\n",
      "(19, 132.53590827121832, 467.0)\n",
      "(20, 132.12530147054113, 406.0)\n",
      "('Training on next batch ', 1, 21)\n",
      "Finished training ...\n",
      "('Evaluation on batch', 2)\n",
      "log_prob (using Gaussian approximation) = -5.947970\n",
      "log_prob (using mixture of Gaussians) = -5.458529\n",
      "Averaging over all trees, mse = 6639.333819, rmse = 81.482107, log_prob = -5.458529\n",
      "(0, 435.83450479799848, 389.0)\n",
      "(1, 437.76494996283867, 414.0)\n",
      "(2, 432.70548328004179, 458.0)\n",
      "(3, 437.06689213177907, 437.0)\n",
      "(4, 435.40874322782179, 409.0)\n",
      "(5, 431.5516441247629, 429.0)\n",
      "(6, 435.7571733261679, 433.0)\n",
      "(7, 432.88294353668022, 431.0)\n",
      "(8, 432.59460336112505, 437.0)\n",
      "(9, 429.81348514705644, 439.0)\n",
      "(10, 430.2152972698878, 456.0)\n",
      "(11, 424.34331298116354, 431.0)\n",
      "(12, 420.71810176248545, 433.0)\n",
      "(13, 421.74081421591285, 383.0)\n",
      "(14, 413.12134805234632, 363.0)\n",
      "(15, 377.98801545528181, 292.0)\n",
      "(16, 387.9561203429447, 285.0)\n",
      "(17, 377.6431060633023, 265.0)\n",
      "(18, 377.0199853014276, 274.0)\n",
      "(19, 375.66514789728228, 278.0)\n",
      "(20, 344.29065410312603, 269.0)\n",
      "('Training on next batch ', 2, 21)\n",
      "Finished training ...\n",
      "('Evaluation on batch', 3)\n",
      "log_prob (using Gaussian approximation) = -5.879628\n",
      "log_prob (using mixture of Gaussians) = -5.986082\n",
      "Averaging over all trees, mse = 7295.944266, rmse = 85.416300, log_prob = -5.986082\n",
      "(0, 279.59080493396084, 245.0)\n",
      "(1, 286.60071513615651, 243.0)\n",
      "(2, 288.8745287906126, 245.0)\n",
      "(3, 302.71982269556668, 241.0)\n",
      "(4, 304.2964459475736, 251.0)\n",
      "(5, 306.58837699512242, 244.0)\n",
      "(6, 308.35994171652817, 235.0)\n",
      "(7, 301.87760499778949, 234.0)\n",
      "(8, 301.84826471103975, 238.0)\n",
      "(9, 303.45411457153534, 252.0)\n",
      "(10, 304.08603658479564, 213.0)\n",
      "(11, 304.66594178520421, 198.0)\n",
      "(12, 308.86109439019043, 193.0)\n",
      "(13, 309.77950020374749, 221.0)\n",
      "(14, 310.18206138724628, 216.0)\n",
      "(15, 310.18130243878556, 206.0)\n",
      "(16, 310.1625697364733, 203.0)\n",
      "(17, 319.57662318242319, 227.0)\n",
      "(18, 319.95491550142356, 203.0)\n",
      "(19, 320.26289914644275, 192.0)\n",
      "(20, 320.76335750842969, 195.0)\n",
      "('Training on next batch ', 3, 21)\n",
      "Finished training ...\n",
      "('Evaluation on batch', 4)\n",
      "log_prob (using Gaussian approximation) = -5.981657\n",
      "log_prob (using mixture of Gaussians) = -5.425928\n",
      "Averaging over all trees, mse = 5171.910499, rmse = 71.915996, log_prob = -5.425928\n",
      "(0, 207.26964226792816, 223.0)\n",
      "(1, 204.973219305697, 179.0)\n",
      "(2, 209.5835383274821, 164.0)\n",
      "(3, 218.57633394921379, 163.0)\n",
      "(4, 216.62403685907123, 192.0)\n",
      "(5, 216.30038507723185, 160.0)\n",
      "(6, 214.06091494462811, 155.0)\n",
      "(7, 246.4818809243404, 141.0)\n",
      "(8, 234.74958554853396, 181.0)\n",
      "(9, 207.86110137270788, 135.0)\n",
      "(10, 212.28638377888257, 135.0)\n",
      "(11, 208.58811425015162, 138.0)\n",
      "(12, 215.63633877644429, 150.0)\n",
      "(13, 214.99146548387364, 128.0)\n",
      "(14, 215.03806781153483, 129.0)\n",
      "(15, 214.19307523390813, 152.0)\n",
      "(16, 221.91896555497905, 129.0)\n",
      "(17, 222.38865764541507, 122.0)\n",
      "(18, 222.92776858696121, 135.0)\n",
      "(19, 223.73421059234303, 142.0)\n",
      "(20, 225.03250214883587, 120.0)\n",
      "('Training on next batch ', 4, 21)\n",
      "Finished training ...\n"
     ]
    }
   ],
   "source": [
    "print '\\nminibatch\\tmetric_train\\tmetric_test\\tnum_leaves'\n",
    "\n",
    "for idx_minibatch in range(settings.n_minibatches):\n",
    "    train_ids_current_minibatch = data['train_ids_partition']['current'][idx_minibatch]\n",
    "        \n",
    "    if idx_minibatch == 0:\n",
    "        with open(settings.data_path + settings.dataset + '.csv', 'w') as f:\n",
    "            f.write(\"target;prediction\\n\")\n",
    "        print(\"Training 0 batch\", len(train_ids_current_minibatch))\n",
    "        # Batch training for first minibatch\n",
    "        mf.fit(data, train_ids_current_minibatch, settings, param, cache)\n",
    "    else:\n",
    "        print('Evaluation on batch', idx_minibatch)\n",
    "        results = mf.evaluate_predictions(data, data['x_train'][train_ids_current_minibatch], data['y_train'][train_ids_current_minibatch], settings, param, weights_prediction, True)\n",
    "        # prediction\n",
    "        predictions = results[0]['pred_mean']\n",
    "        real = data['y_train'][train_ids_current_minibatch].flatten()\n",
    "        for i in range(len(predictions)):\n",
    "            print(i, predictions[i], real[i])\n",
    "            with open(settings.data_path + settings.dataset + '.csv', 'a') as f:\n",
    "                f.write(\"{0},{1}\\n\".format(real[i], predictions[i]))        \n",
    "        \n",
    "        print(\"Training on next batch \", idx_minibatch, len(train_ids_current_minibatch))\n",
    "        # Online update\n",
    "        mf.partial_fit(data, train_ids_current_minibatch, settings, param, cache)\n",
    "    print(\"Finished training ...\")\n",
    "    # Evaluate\n",
    "    weights_prediction = np.ones(settings.n_mondrians) * 1.0 / settings.n_mondrians\n",
    "    train_ids_cumulative = data['train_ids_partition']['cumulative'][idx_minibatch]        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_current_minibatch = data['train_ids_partition']['current'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_prob (using Gaussian approximation) = -13.773920\n",
      "log_prob (using mixture of Gaussians) = -3.970565\n",
      "Averaging over all trees, mse = 9632.335904, rmse = 98.144464, log_prob = -3.970565\n"
     ]
    }
   ],
   "source": [
    "results = mf.evaluate_predictions(data, data['x_train'][train_ids_current_minibatch], data['y_train'][train_ids_current_minibatch], settings, param, weights_prediction, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results[0]['pred_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 127.91568796733891, 125.0)\n",
      "(1, 129.44538569517289, 160.0)\n",
      "(2, 133.85071834754064, 123.0)\n",
      "(3, 130.02109834833138, 129.0)\n",
      "(4, 133.19403165301617, 146.0)\n",
      "(5, 134.65254759163403, 142.0)\n",
      "(6, 132.79489574260475, 134.0)\n",
      "(7, 134.31670238830841, 151.0)\n",
      "(8, 133.93475589312868, 123.0)\n",
      "(9, 132.59401928162981, 151.0)\n",
      "(10, 132.45261667794637, 134.0)\n",
      "(11, 154.28040897599456, 161.0)\n",
      "(12, 174.26108590654275, 182.0)\n",
      "(13, 174.25470779687242, 176.0)\n",
      "(14, 174.42811032533834, 207.0)\n",
      "(15, 174.05352287268602, 215.0)\n",
      "(16, 229.90753383299625, 274.0)\n",
      "(17, 258.90459639067166, 324.0)\n",
      "(18, 267.56513898966926, 322.0)\n",
      "(19, 301.22887622171891, 325.0)\n",
      "(20, 304.02803637777379, 361.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictions)):\n",
    "    print(i, predictions[i], real[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = data['y_train'][train_ids_current_minibatch].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.079,  0.   ,  0.   ,  0.   ,  0.054,  0.281,  0.73 ,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.384,  0.678,  0.857,  0.683,  0.201,  0.216,  0.   ,  1.   ,  0.262,  0.   ,  0.957,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.079,  0.079,  0.054,  0.248,  0.73 ,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.287,  0.678,  0.856,  0.683,  0.203,  0.216,  0.   ,  0.924,  0.26 ,  0.   ,  0.957,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.054,  0.224,  0.73 ,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.267,  0.678,  0.856,  0.683,  0.205,  0.216,  0.   ,  0.847,  0.259,  0.   ,  0.957,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.079,  0.   ,  0.   ,  0.054,  0.207,  0.733,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.249,  0.674,  0.856,  0.683,  0.208,  0.21 ,  0.   ,  0.742,  0.249,  0.   ,  0.957,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.079,  0.079,  0.054,  0.191,  0.73 ,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.248,  0.678,  0.856,  0.683,  0.211,  0.205,  0.   ,  0.636,  0.24 ,  0.   ,  1.   ,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.   ,  0.   ,  0.054,  0.175,  0.733,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.224,  0.674,  0.856,  0.683,  0.208,  0.216,  0.   ,  0.606,  0.248,  0.   ,  1.   ,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.079,  0.   ,  0.   ,  0.036,  0.155,  0.733,  0.876,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.202,  0.674,  0.856,  0.683,  0.205,  0.227,  0.   ,  0.576,  0.256,  0.   ,  1.   ,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.079,  0.079,  0.054,  0.142,  0.735,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.186,  0.671,  0.856,  0.683,  0.205,  0.222,  0.   ,  0.547,  0.248,  0.   ,  1.   ,\n",
       "         0.333,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.036,  0.122,  0.733,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.156,  0.674,  0.856,  0.683,  0.205,  0.216,  0.   ,  0.517,  0.24 ,  0.   ,  0.   ,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.079,  0.   ,  0.   ,  0.054,  0.102,  0.735,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.096,  0.671,  0.856,  0.683,  0.203,  0.216,  0.   ,  0.525,  0.249,  0.   ,  0.   ,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.079,  0.079,  0.036,  0.081,  0.735,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.055,  0.671,  0.856,  0.683,  0.201,  0.216,  0.   ,  0.534,  0.259,  0.   ,  0.   ,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.   ,  0.   ,  0.054,  0.073,  0.735,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.043,  0.671,  0.855,  0.683,  0.199,  0.216,  0.   ,  0.564,  0.262,  0.   ,  0.   ,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.079,  0.   ,  0.   ,  0.036,  0.061,  0.735,  0.877,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.03 ,  0.671,  0.856,  0.683,  0.197,  0.216,  0.   ,  0.593,  0.265,  0.   ,  0.043,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.079,  0.   ,  0.054,  0.053,  0.738,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.667,  0.855,  0.683,  0.193,  0.222,  0.   ,  0.597,  0.263,  0.   ,  0.043,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.079,  0.036,  0.053,  0.735,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.671,  0.855,  0.683,  0.19 ,  0.227,  0.   ,  0.602,  0.262,  0.   ,  0.043,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.079,  0.   ,  0.   ,  0.054,  0.053,  0.738,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.667,  0.855,  0.683,  0.186,  0.244,  0.   ,  0.585,  0.258,  0.   ,  0.043,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.079,  0.   ,  0.036,  0.053,  0.738,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.667,  0.855,  0.683,  0.182,  0.261,  0.   ,  0.568,  0.253,  0.   ,  0.087,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.   ,  0.079,  0.054,  0.053,  0.738,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.667,  0.855,  0.683,  0.18 ,  0.261,  0.   ,  0.585,  0.256,  0.   ,  0.087,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.079,  0.   ,  0.   ,  0.054,  0.057,  0.74 ,  0.879,  0.942,  0.   ,  0.   ,  0.857,  0.855,  0.015,  0.664,  0.854,  0.683,  0.178,  0.261,  0.   ,  0.602,  0.259,  0.   ,  0.087,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.079,  0.   ,  0.054,  0.053,  0.74 ,  0.878,  0.941,  0.   ,  0.   ,  0.857,  0.855,  0.016,  0.664,  0.855,  0.683,  0.178,  0.261,  0.   ,  0.631,  0.258,  0.   ,  0.087,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ],\n",
       "       [ 0.079,  0.   ,  0.   ,  0.079,  0.054,  0.057,  0.74 ,  0.879,  0.942,  0.   ,  0.   ,  0.857,  0.855,  0.015,  0.664,  0.854,  0.683,  0.178,  0.261,  0.   ,  0.661,  0.256,  0.   ,  0.13 ,\n",
       "         0.5  ,  0.091,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x_train'][train_ids_current_minibatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_sparse': False,\n",
       " 'n_dim': 34,\n",
       " 'n_test': 21062,\n",
       " 'n_train': 84246,\n",
       " 'train_ids_partition': {'cumulative': {0: array([    0,     1,     2, ..., 28079, 28080, 28081]),\n",
       "   1: array([    0,     1,     2, ..., 56161, 56162, 56163]),\n",
       "   2: array([    0,     1,     2, ..., 84243, 84244, 84245])},\n",
       "  'current': {0: array([    0,     1,     2, ..., 28079, 28080, 28081]),\n",
       "   1: array([28082, 28083, 28084, ..., 56161, 56162, 56163]),\n",
       "   2: array([56164, 56165, 56166, ..., 84243, 84244, 84245])}},\n",
       " 'x_test': array([[ 0.079,  0.079,  0.157, ...,  1.   ,  0.667,  0.333],\n",
       "        [ 0.157,  0.079,  0.   , ...,  1.   ,  0.667,  0.333],\n",
       "        [ 0.157,  0.   ,  0.   , ...,  1.   ,  0.667,  0.333],\n",
       "        ..., \n",
       "        [ 0.055,  0.055,  0.047, ...,  0.   ,  0.997,  0.   ],\n",
       "        [ 0.047,  0.055,  0.055, ...,  0.   ,  0.997,  0.   ],\n",
       "        [ 0.055,  0.047,  0.047, ...,  0.   ,  0.997,  0.   ]], dtype=float32),\n",
       " 'x_train': array([[ 0.   ,  0.   ,  0.   , ...,  1.   ,  1.   ,  0.   ],\n",
       "        [ 0.079,  0.079,  0.079, ...,  1.   ,  1.   ,  0.   ],\n",
       "        [ 0.   ,  0.   ,  0.   , ...,  1.   ,  1.   ,  0.   ],\n",
       "        ..., \n",
       "        [ 0.079,  0.315,  0.079, ...,  1.   ,  0.667,  0.333],\n",
       "        [ 0.157,  0.315,  0.   , ...,  1.   ,  0.667,  0.333],\n",
       "        [ 0.157,  0.236,  0.   , ...,  1.   ,  0.667,  0.333]], dtype=float32),\n",
       " 'y_test': array([[ 1. ],\n",
       "        [ 2. ],\n",
       "        [ 2. ],\n",
       "        ..., \n",
       "        [ 0.7],\n",
       "        [ 0.8],\n",
       "        [ 0.7]], dtype=float32),\n",
       " 'y_train': array([[ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        ..., \n",
       "        [ 2.],\n",
       "        [ 2.],\n",
       "        [ 2.]], dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
